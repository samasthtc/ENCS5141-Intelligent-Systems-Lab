{"cells":[{"cell_type":"markdown","source":["# Loading the Dataset"],"metadata":{"id":"VQzHK-y_vxNR"}},{"cell_type":"code","source":["import pandas as pd\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import classification_report\n","from sklearn.preprocessing import LabelEncoder\n","\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier"],"metadata":{"id":"aKBVZkOgi07O","executionInfo":{"status":"ok","timestamp":1723052574059,"user_tz":-180,"elapsed":341,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["pd.options.display.max_columns = 999"],"metadata":{"id":"5tCh0HUPgy70","executionInfo":{"status":"ok","timestamp":1723052574508,"user_tz":-180,"elapsed":1,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["After completing Case Study #1 and saving the data after encoding and the final cleaned data to “encoded_data.csv” and “PCA_data.csv” respectively, they’ll be used to split the dataset and train the models."],"metadata":{"id":"u_aZEPIzz7p9"}},{"cell_type":"code","source":["PCA_data = pd.read_csv(\"./PCA_data.csv\")\n","encoded_data = pd.read_csv(\"./encoded_data.csv\")"],"metadata":{"id":"kPUB72Pyv1FX","collapsed":true,"executionInfo":{"status":"ok","timestamp":1723052574846,"user_tz":-180,"elapsed":339,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["# Categorizing and Splitting the Data"],"metadata":{"id":"fxhIIS_aZeIm"}},{"cell_type":"markdown","source":["The aim is to train the three models to categorize/classify bike demand as \"extreme,\" \"high,\" \"medium,\" or \"low\" based on rental bike counts. To do this, the data after PCA needs to be categorized into bins in the ranges of [0, 0.25, 0.5, 0.75, 1.0] and labels ['low', 'medium', 'high', 'extreme']. Then the data can be split for training and testing."],"metadata":{"id":"QPiutRKUz-Ff"}},{"cell_type":"code","source":["PCA_X = PCA_data\n","PCA_y = encoded_data['count']\n","\n","bins = [0, 0.25, 0.5, 0.75, 1.0]\n","labels = ['low', 'medium', 'high', 'extreme']\n","\n","label_encoder = LabelEncoder()\n","PCA_y_binned = label_encoder.fit_transform(pd.cut(PCA_y, bins=bins, labels=labels))\n","\n","PCA_X_train, PCA_X_test, PCA_y_train, PCA_y_test = train_test_split(PCA_X, PCA_y_binned, test_size=0.2, random_state=42)\n"],"metadata":{"id":"Zvdy0l_hVJyP","executionInfo":{"status":"ok","timestamp":1723052574847,"user_tz":-180,"elapsed":3,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["The number of instances for each category:"],"metadata":{"id":"L_inOJ330Gf2"}},{"cell_type":"code","source":["binned_counts = pd.cut(encoded_data['count'], bins=bins, labels=labels)\n","print(binned_counts.value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2SH7nbahnHi","executionInfo":{"status":"ok","timestamp":1723067276587,"user_tz":-180,"elapsed":907,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}},"outputId":"8e0d032d-6f2a-4bb4-c355-56465665187c"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["count\n","low        12105\n","medium      3782\n","high        1375\n","extreme        0\n","Name: count, dtype: int64\n"]}]},{"cell_type":"markdown","source":["# Hyperparameter Tuning and Training"],"metadata":{"id":"CPrQGyOUad_O"}},{"cell_type":"markdown","source":["To find the best hyperparameters to use for each model, cross validation using Grid Search will be utilized with pre-defined lists of parameters for each model. The Random Forest Classifier will be tuned for the parameters [n_estimators, max_depth, min_samples_split]. The XGBoost model will be tuned for the following [n_estimators, max_depth, learning_rate]. The MLP model will be tuned for [hidden_layer_sizes, activation, solver, learning_rate]."],"metadata":{"id":"0sTk3GVm0J1o"}},{"cell_type":"code","source":["rf = RandomForestClassifier()\n","xgb = XGBClassifier()\n","mlp = MLPClassifier()\n","\n","rf_params = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [None, 10, 20, 30],\n","    'min_samples_split': [2, 5, 10]\n","}\n","\n","xgb_params = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [3, 6, 9],\n","    'learning_rate': [0.01, 0.1, 0.2]\n","}\n","\n","mlp_params = {\n","    'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n","    'activation': ['relu', 'tanh'],\n","    'solver': ['adam', 'sgd'],\n","    'learning_rate': ['constant', 'adaptive']\n","}"],"metadata":{"id":"1wpxHPB-atRR","executionInfo":{"status":"ok","timestamp":1723052574847,"user_tz":-180,"elapsed":2,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["To perform Grid Search cross-validation and train the data on the best hyperparameters for each model:"],"metadata":{"id":"LBfCGlls0N_q"}},{"cell_type":"code","source":["def perform_grid_search(model, params, X_train, y_train):\n","    grid_search = GridSearchCV(estimator=model, param_grid=params,\n","                               cv=5, scoring='accuracy', n_jobs=-1)\n","    grid_search.fit(X_train, y_train)\n","    return grid_search.best_estimator_, grid_search.best_params_\n","\n","best_rf, best_rf_params = perform_grid_search(rf, rf_params, PCA_X_train, PCA_y_train)\n","best_xgb, best_xgb_params = perform_grid_search(xgb, xgb_params, PCA_X_train, PCA_y_train)\n","best_mlp, best_mlp_params = perform_grid_search(mlp, mlp_params, PCA_X_train, PCA_y_train)\n"],"metadata":{"id":"z8I93AAkagVt","executionInfo":{"status":"ok","timestamp":1723055291761,"user_tz":-180,"elapsed":2713399,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["print(\"Best Random Forest Parameters:\", best_rf_params)\n","print(\"Best XGBoost Parameters:\", best_xgb_params)\n","print(\"Best MLP Parameters:\", best_mlp_params)"],"metadata":{"id":"z8EIBMw2LaY7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723055674882,"user_tz":-180,"elapsed":351,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}},"outputId":"508cb831-3959-478d-e4db-d81910b904fa"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Random Forest Parameters: {'max_depth': 30, 'min_samples_split': 5, 'n_estimators': 100}\n","Best XGBoost Parameters: {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 200}\n","Best MLP Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100, 50), 'learning_rate': 'adaptive', 'solver': 'adam'}\n"]}]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"fmLta0-HW_11"}},{"cell_type":"markdown","source":["Now that each model is trained on the data using the best hyperparameters from the given grids, testing them will be conducted through the following code."],"metadata":{"id":"Er-6BO2N0Rmk"}},{"cell_type":"code","source":["# Evaluate the models and print the best parameters\n","def evaluate_model(model, X_test, y_test):\n","    y_pred = model.predict(X_test)\n","    unique_labels = label_encoder.inverse_transform(sorted(set(y_test)))\n","    report = classification_report(y_test, y_pred, target_names=unique_labels)\n","    print(report)\n","\n","print(\"Random Forest Evaluation:\")\n","evaluate_model(best_rf, PCA_X_test, PCA_y_test)\n","\n","print(\"XGBoost Evaluation:\")\n","evaluate_model(best_xgb, PCA_X_test, PCA_y_test)\n","\n","print(\"MLP Evaluation:\")\n","evaluate_model(best_mlp, PCA_X_test, PCA_y_test)"],"metadata":{"id":"bL-osWeXgg5n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723056004821,"user_tz":-180,"elapsed":832,"user":{"displayName":"Osama Shoora","userId":"06055927753721480644"}},"outputId":"782cd007-255c-4fa4-e5c5-4f95354e2cce"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Evaluation:\n","              precision    recall  f1-score   support\n","\n","        high       0.88      0.83      0.85       290\n","         low       0.99      0.99      0.99      2420\n","      medium       0.91      0.92      0.91       743\n","\n","    accuracy                           0.96      3453\n","   macro avg       0.93      0.91      0.92      3453\n","weighted avg       0.96      0.96      0.96      3453\n","\n","XGBoost Evaluation:\n","              precision    recall  f1-score   support\n","\n","        high       0.89      0.84      0.86       290\n","         low       0.99      0.99      0.99      2420\n","      medium       0.91      0.93      0.92       743\n","\n","    accuracy                           0.97      3453\n","   macro avg       0.93      0.92      0.93      3453\n","weighted avg       0.97      0.97      0.97      3453\n","\n","MLP Evaluation:\n","              precision    recall  f1-score   support\n","\n","        high       0.85      0.83      0.84       290\n","         low       1.00      1.00      1.00      2420\n","      medium       0.93      0.93      0.93       743\n","\n","    accuracy                           0.97      3453\n","   macro avg       0.93      0.92      0.92      3453\n","weighted avg       0.97      0.97      0.97      3453\n","\n"]}]},{"cell_type":"markdown","source":["All three models show high accuracy, with **RF** at 96%, **XGBoost** at 97%, and **MLP** at 97%. This suggests that the models are generally suitable for predicting bike rental demand categories based on the PCA-transformed features. The high precision and recall for the 'low' category across all models indicate that the models are especially good at predicting low bike rental demand, which is also the majority class (support = 2420).\n","\n","</br>\n","\n","The '*high*' and '*medium*' categories have slightly lower **precision** and **recall** than the '*low*' category. This is anticipated, given the lower support (number of instances) for these classes (290 for high and 743 for medium). The **F1-scores** for the '*high*' and '*medium*' categories remain quite high, demonstrating that the models perform well even with fewer examples.\n","\n","</br>\n","\n","**Random Forest** performs well with high **accuracy** and balanced **precision** and **recall** across categories. However, it has somewhat worse **precision** and **recall** in the '*high*' category than **XGBoost** and **MLP**. </br>\n","\n","**XGBoost** slightly surpasses **Random Forest** in terms of **accuracy** and **precision/recall** in the '*high*' and '*medium*' categories. This could be attributed to **XGBoost**'s capacity to handle complicated relationships and interactions between features. </br>\n","\n","**MLP** performs similarly to **XGBoost**, with flawless **precision** and **recall** for the '*low*' category and high scores in the '*medium*' and '*high*' categories. The **MLP**'s performance demonstrates that neural networks can effectively detect patterns in data post PCA transformation.\n","\n","</br>\n","\n","It is worth mentioning that the *‘extreme’* category did not show up in the evaluation metrics, and this is due to it having no instances in the data based on the selected ranges."],"metadata":{"id":"vTgZvNC_0Twk"}}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}